"""
    COMPLETE implementation of custom metrics for prompt evaluation.
    CHALLENGE SOLUTION

    This module implements general and specific metrics for Bug to User Story:

    GENERAL METRICS (3):
    1. F1-Score: Balance between Precision and Recall
    2. Clarity: Response clarity and structure
    3. Precision: Correct and relevant information

    SPECIFIC METRICS FOR BUG TO USER STORY (4):
    4. Tone Score: Professional and empathetic tone
    5. Acceptance Criteria Score: Quality of acceptance criteria
    6. User Story Format Score: Correct format (As a... I want... So that...)
    7. Completeness Score: Completeness and technical context

    Supports multiple LLM providers:
    - OpenAI (gpt-5.2)
    - Google Gemini (gemini-1.5-flash, gemini-1.5-pro)

    Configure the provider in the .env file through the LLM_PROVIDER variable.
"""

import os
import json
import re
from typing import Dict, Any
from dotenv import load_dotenv
from langchain_core.messages import SystemMessage, HumanMessage
from utils import get_eval_llm

load_dotenv()


def get_evaluator_llm():
    """
    Returns the LLM configured for evaluation.
    Supports OpenAI and Google Gemini based on .env
    """
    return get_eval_llm()


def extract_json_from_response(response_text: str) -> Dict[str, Any]:
    """
    Extracts JSON from an LLM response that may contain additional text.
    """
    try:
        # Try to parse directly
        return json.loads(response_text)
    except json.JSONDecodeError:
        # Try to find JSON in the middle of the text
        start = response_text.find('{')
        end = response_text.rfind('}') + 1

        if start != -1 and end > start:
            try:
                json_str = response_text[start:end]
                return json.loads(json_str)
            except json.JSONDecodeError:
                pass

        # If unable to extract, return default values
        print(f"Could not extract JSON from response: {response_text[:200]}...")
        return {"score": 0.0, "reasoning": "Error processing response"}


def evaluate_f1_score(question: str, answer: str, reference: str) -> Dict[str, Any]:
    """
    Calculates F1-Score using LLM-as-Judge.

    F1-Score = 2 * (Precision * Recall) / (Precision + Recall)

    Args:
        question: Question asked by the user
        answer: Answer generated by the prompt
        reference: Expected answer (ground truth)

    Returns:
        Dict with score and reasoning:
        {
            "score": 0.95,
            "precision": 0.9,
            "recall": 0.99,
            "reasoning": "LLM explanation..."
        }
    """
    evaluator_prompt = f"""
    You are an evaluator specialized in measuring the quality of AI-generated responses.

    Your task is to calculate PRECISION and RECALL to determine the F1-Score.

    USER QUESTION:
    {question}

    EXPECTED ANSWER (Ground Truth):
    {reference}

    ANSWER GENERATED BY MODEL:
    {answer}

    INSTRUCTIONS:

    1. PRECISION (0.0 to 1.0):
    - How much information in the generated answer is CORRECT and RELEVANT?
    - Penalize incorrect, invented or unnecessary information
    - 1.0 = all information is correct and relevant
    - 0.0 = no information is correct or relevant

    2. RECALL (0.0 to 1.0):
    - How much information from the expected answer is PRESENT in the generated answer?
    - Penalize important information that was omitted
    - 1.0 = all important information is present
    - 0.0 = no important information is present

    3. REASONING:
    - Briefly explain your evaluation
    - Cite specific examples of what was correct/incorrect

    IMPORTANT: Return ONLY a valid JSON object in the format:
    {{
    "precision": <value between 0.0 and 1.0>,
    "recall": <value between 0.0 and 1.0>,
    "reasoning": "<your explanation in up to 100 words>"
    }}

    DO NOT add any text before or after the JSON.
    """

    try:
        llm = get_evaluator_llm()
        response = llm.invoke([HumanMessage(content=evaluator_prompt)])
        result = extract_json_from_response(response.content)

        precision = float(result.get("precision", 0.0))
        recall = float(result.get("recall", 0.0))

        # Calculate F1-Score
        if (precision + recall) > 0:
            f1_score = 2 * (precision * recall) / (precision + recall)
        else:
            f1_score = 0.0

        return {
            "score": round(f1_score, 4),
            "precision": round(precision, 4),
            "recall": round(recall, 4),
            "reasoning": result.get("reasoning", "")
        }

    except Exception as e:
        print(f"Error evaluating F1-Score: {e}")
        return {
            "score": 0.0,
            "precision": 0.0,
            "recall": 0.0,
            "reasoning": f"Evaluation error: {str(e)}"
        }


def evaluate_clarity(question: str, answer: str, reference: str) -> Dict[str, Any]:
    """
    Evaluates the clarity and structure of the answer using LLM-as-Judge.

    Criteria:
    - Clear organization and structure
    - Simple and direct language
    - Absence of ambiguity
    - Easy to understand

    Args:
        question: Question asked by the user
        answer: Answer generated by the prompt
        reference: Expected answer (ground truth)

    Returns:
        Dict with score and reasoning:
        {
            "score": 0.92,
            "reasoning": "LLM explanation..."
        }
    """
    evaluator_prompt = f"""
    You are an evaluator specialized in measuring the CLARITY of AI-generated responses.

    USER QUESTION:
    {question}

    ANSWER GENERATED BY MODEL:
    {answer}

    EXPECTED ANSWER (Reference):
    {reference}

    INSTRUCTIONS:

    Evaluate the CLARITY of the generated answer based on the criteria:

    1. ORGANIZATION (0.0 to 1.0):
    - Does the answer have logical and well-organized structure?
    - Is information in a sensible order?

    2. LANGUAGE (0.0 to 1.0):
    - Uses simple and direct language?
    - Avoids unnecessary jargon?
    - Easy to understand?

    3. ABSENCE OF AMBIGUITY (0.0 to 1.0):
    - Is the answer clear and unambiguous?
    - Leaves no doubts about what is being communicated?

    4. CONCISENESS (0.0 to 1.0):
    - Is it concise without being too short?
    - No redundant information?

    Calculate the AVERAGE of the 4 criteria to obtain the final score.

    IMPORTANT: Return ONLY a valid JSON object in the format:
    {{
    "score": <value between 0.0 and 1.0>,
    "reasoning": "<detailed explanation of the evaluation in up to 100 words>"
    }}

    DO NOT add any text before or after the JSON.
    """

    try:
        llm = get_evaluator_llm()
        response = llm.invoke([HumanMessage(content=evaluator_prompt)])
        result = extract_json_from_response(response.content)

        score = float(result.get("score", 0.0))

        return {
            "score": round(score, 4),
            "reasoning": result.get("reasoning", "")
        }

    except Exception as e:
        print(f"Error evaluating Clarity: {e}")
        return {
            "score": 0.0,
            "reasoning": f"Evaluation error: {str(e)}"
        }


def evaluate_precision(question: str, answer: str, reference: str) -> Dict[str, Any]:
    """
    Evaluates the precision of the answer using LLM-as-Judge.

    Criteria:
    - Absence of invented information (hallucinations)
    - Answer focused on the question
    - Correct and verifiable information

    Args:
        question: Question asked by the user
        answer: Answer generated by the prompt
        reference: Expected answer (ground truth)

    Returns:
        Dict with score and reasoning:
        {
            "score": 0.98,
            "reasoning": "LLM explanation..."
        }
    """
    
    evaluator_prompt = f"""
    You are an evaluator specialized in detecting PRECISION and HALLUCINATIONS in AI responses.

    USER QUESTION:
    {question}

    ANSWER GENERATED BY MODEL:
    {answer}

    EXPECTED ANSWER (Ground Truth):
    {reference}

    INSTRUCTIONS:

    Evaluate the PRECISION of the generated answer:

    1. ABSENCE OF HALLUCINATIONS (0.0 to 1.0):
    - Does the answer contain INVENTED or unverifiable information?
    - Are all statements based on facts?
    - 1.0 = no hallucination detected
    - 0.0 = answer full of invented information

    2. FOCUS ON QUESTION (0.0 to 1.0):
    - Does the answer respond EXACTLY to what was asked?
    - Doesn't ramble or add unsolicited information?
    - 1.0 = totally focused
    - 0.0 = completely off-topic

    3. FACTUAL CORRECTNESS (0.0 to 1.0):
    - Is the information CORRECT when compared to the reference?
    - No errors or inaccuracies?
    - 1.0 = all information correct
    - 0.0 = incorrect information

    Calculate the AVERAGE of the 3 criteria to obtain the final score.

    IMPORTANT: Return ONLY a valid JSON object in the format:
    {{
    "score": <value between 0.0 and 1.0>,
    "reasoning": "<detailed explanation in up to 100 words, cite examples>"
    }}

    DO NOT add any text before or after the JSON.
    """

    try:
        llm = get_evaluator_llm()
        response = llm.invoke([HumanMessage(content=evaluator_prompt)])
        result = extract_json_from_response(response.content)

        score = float(result.get("score", 0.0))

        return {
            "score": round(score, 4),
            "reasoning": result.get("reasoning", "")
        }

    except Exception as e:
        print(f"Error evaluating Precision: {e}")
        return {
            "score": 0.0,
            "reasoning": f"Evaluation error: {str(e)}"
        }


def evaluate_tone_score(bug_report: str, user_story: str, reference: str) -> Dict[str, Any]:
    """
    Evaluates the tone of the user story (professional and empathetic).

    Specific criteria for Bug to User Story:
    - Professional but not overly technical tone
    - Empathy with the user affected by the bug
    - Focus on business value, not just technical correction
    - Positive language (what the user WANTS to do, not just what doesn't work)

    Args:
        bug_report: Original bug description
        user_story: User story generated by the prompt
        reference: Expected user story (ground truth)

    Returns:
        Dict with score and reasoning
    """
    evaluator_prompt = f"""
    You are an evaluator specialized in Agile User Stories.

    ORIGINAL BUG REPORT:
    {bug_report}

    GENERATED USER STORY:
    {user_story}

    EXPECTED USER STORY (Reference):
    {reference}

    INSTRUCTIONS:

    Evaluate the TONE of the generated user story based on the criteria:

    1. PROFESSIONALISM (0.0 to 1.0):
    - Uses professional and appropriate language for documentation?
    - Avoids excessive jargon or overly informal language?
    - Maintains agile documentation quality standard?

    2. USER EMPATHY (0.0 to 1.0):
    - Demonstrates understanding of the bug's impact on the user?
    - Focuses on user's need/frustration?
    - Uses user-centered language ("As a... I want...")?

    3. VALUE FOCUS (0.0 to 1.0):
    - Clearly articulates the business value of the solution?
    - Goes beyond "fixing the bug" and explains the benefit?
    - Uses the structure "so that I can..." with real value?

    4. POSITIVE LANGUAGE (0.0 to 1.0):
    - Focuses on what the user WANTS to do (not just what's broken)?
    - Constructive and solution-oriented tone?
    - Avoids negative or blaming language?

    Calculate the AVERAGE of the 4 criteria to obtain the final score.

    IMPORTANT: Return ONLY a valid JSON object in the format:
    {{
    "score": <value between 0.0 and 1.0>,
    "reasoning": "<detailed explanation in up to 150 words>"
    }}

    DO NOT add any text before or after the JSON.
    """

    try:
        llm = get_evaluator_llm()
        response = llm.invoke([HumanMessage(content=evaluator_prompt)])
        result = extract_json_from_response(response.content)

        score = float(result.get("score", 0.0))

        return {
            "score": round(score, 4),
            "reasoning": result.get("reasoning", "")
        }

    except Exception as e:
        print(f"Error evaluating Tone Score: {e}")
        return {
            "score": 0.0,
            "reasoning": f"Evaluation error: {str(e)}"
        }


def evaluate_acceptance_criteria_score(bug_report: str, user_story: str, reference: str) -> Dict[str, Any]:
    """
    Evaluates the quality of acceptance criteria.

    Specific criteria:
    - Uses Given-When-Then format or similar structured approach
    - Criteria are specific and testable
    - Adequate quantity (3-7 criteria ideally)
    - Complete coverage of bug and solution
    - Includes edge case scenarios when relevant

    Args:
        bug_report: Original bug description
        user_story: User story generated by the prompt
        reference: Expected user story (ground truth)

    Returns:
        Dict with score and reasoning
    """
    evaluator_prompt = f"""
    You are an evaluator specialized in User Story Acceptance Criteria.

    ORIGINAL BUG REPORT:
    {bug_report}

    GENERATED USER STORY:
    {user_story}

    EXPECTED USER STORY (Reference):
    {reference}

    INSTRUCTIONS:

    Evaluate the ACCEPTANCE CRITERIA of the generated user story:

    1. STRUCTURED FORMAT (0.0 to 1.0):
    - Uses Given-When-Then format or similar structure?
    - Each criterion is clearly separated and identifiable?
    - Formatting facilitates reading and understanding?

    2. SPECIFICITY AND TESTABILITY (0.0 to 1.0):
    - Are criteria specific and not vague?
    - Is it possible to create automated tests from them?
    - Avoids ambiguous terms like "should work well"?
    - Measurable and verifiable criteria?

    3. ADEQUATE QUANTITY (0.0 to 1.0):
    - Has appropriate number of criteria (not too many, not too few)?
    - Ideal: 3-7 criteria for simple/medium bugs
    - Complex bugs may have more organized criteria

    4. COMPLETE COVERAGE (0.0 to 1.0):
    - Covers all aspects of the bug?
    - Includes success and error scenarios?
    - Considers edge cases when relevant?
    - Addresses validations and technical requirements of the bug?

    Calculate the AVERAGE of the 4 criteria to obtain the final score.

    IMPORTANT: Return ONLY a valid JSON object in the format:
    {{
    "score": <value between 0.0 and 1.0>,
    "reasoning": "<detailed explanation with specific examples, up to 150 words>"
    }}

    DO NOT add any text before or after the JSON.
    """

    try:
        llm = get_evaluator_llm()
        response = llm.invoke([HumanMessage(content=evaluator_prompt)])
        result = extract_json_from_response(response.content)

        score = float(result.get("score", 0.0))

        return {
            "score": round(score, 4),
            "reasoning": result.get("reasoning", "")
        }

    except Exception as e:
        print(f"Error evaluating Acceptance Criteria Score: {e}")
        return {
            "score": 0.0,
            "reasoning": f"Evaluation error: {str(e)}"
        }


def evaluate_user_story_format_score(bug_report: str, user_story: str, reference: str) -> Dict[str, Any]:
    """
    Evaluates if the user story follows the correct standard format.

    Expected format:
    - "As a [type of user]"
    - "I want [action/functionality]"
    - "So that [benefit/value]"
    - Acceptance Criteria clearly separated

    Args:
        bug_report: Original bug description
        user_story: User story generated by the prompt
        reference: Expected user story (ground truth)

    Returns:
        Dict with score and reasoning
    """
    evaluator_prompt = f"""
    You are an evaluator specialized in Agile User Story format.

    ORIGINAL BUG REPORT:
    {bug_report}

    GENERATED USER STORY:
    {user_story}

    EXPECTED USER STORY (Reference):
    {reference}

    INSTRUCTIONS:

    Evaluate the FORMAT of the generated user story:

    1. STANDARD TEMPLATE (0.0 to 1.0):
    - Follows the format "As a [user], I want [action], so that [benefit]"?
    - Are all three parts present and correct?
    - Do order and structure follow best practices?

    2. PERSONA IDENTIFICATION (0.0 to 1.0):
    - Does "As a..." clearly identify the type of user?
    - Is the persona specific and relevant to the bug?
    - Avoids generics like "As a user" without context?

    3. CLEAR ACTION (0.0 to 1.0):
    - Does "I want..." clearly describe the desired action/functionality?
    - Is the action specific and related to the bug?
    - Avoids vague or overly technical descriptions?

    4. ARTICULATED BENEFIT (0.0 to 1.0):
    - Does "So that..." clearly explain the value/benefit?
    - Is the benefit real and significant (not trivial)?
    - Connects the action to business value?

    5. SECTION SEPARATION (0.0 to 1.0):
    - Is the main user story clearly separated from the criteria?
    - Do acceptance criteria have their own section?
    - Does structure facilitate reading and navigation?

    Calculate the AVERAGE of the 5 criteria to obtain the final score.

    IMPORTANT: Return ONLY a valid JSON object in the format:
    {{
    "score": <value between 0.0 and 1.0>,
    "reasoning": "<detailed explanation with examples, up to 150 words>"
    }}

    DO NOT add any text before or after the JSON.
    """

    try:
        llm = get_evaluator_llm()
        response = llm.invoke([HumanMessage(content=evaluator_prompt)])
        result = extract_json_from_response(response.content)

        score = float(result.get("score", 0.0))

        return {
            "score": round(score, 4),
            "reasoning": result.get("reasoning", "")
        }

    except Exception as e:
        print(f"Error evaluating User Story Format Score: {e}")
        return {
            "score": 0.0,
            "reasoning": f"Evaluation error: {str(e)}"
        }


def evaluate_completeness_score(bug_report: str, user_story: str, reference: str) -> Dict[str, Any]:
    """
    Evaluates the completeness of the user story in relation to the bug.

    Specific criteria based on bug complexity:
    - Simple bugs: covers basic problem
    - Medium bugs: includes relevant technical context
    - Complex bugs: addresses multiple aspects, impact, technical tasks

    Args:
        bug_report: Original bug description
        user_story: User story generated by the prompt
        reference: Expected user story (ground truth)

    Returns:
        Dict with score and reasoning
    """
    evaluator_prompt = f"""
    You are an evaluator specialized in completeness of User Stories derived from bugs.

    ORIGINAL BUG REPORT:
    {bug_report}

    GENERATED USER STORY:
    {user_story}

    EXPECTED USER STORY (Reference):
    {reference}

    INSTRUCTIONS:

    Evaluate the COMPLETENESS of the user story in relation to the bug:

    1. PROBLEM COVERAGE (0.0 to 1.0):
    - Does the user story address ALL aspects of the reported bug?
    - No important details were omitted?
    - If bug mentions multiple problems, are all covered?

    2. TECHNICAL CONTEXT (0.0 to 1.0):
    - When the bug includes technical details (logs, stack traces, endpoints):
        * Does user story preserve relevant technical context?
        * Is technical information included appropriately?
    - Simple bugs don't need much technical context
    - Complex bugs MUST include technical context section

    3. IMPACT AND SEVERITY (0.0 to 1.0):
    - If the bug mentions impact (affected users, financial loss):
        * Does user story recognize and document the impact?
    - Is severity reflected in implicit prioritization?
    - Critical bugs should have more detailed treatment

    4. TECHNICAL TASKS (0.0 to 1.0):
    - For complex bugs with multiple components:
        * Does user story suggest technical tasks or breakdown?
    - For simple/medium bugs:
        * Tasks are not necessary (don't penalize absence)
    - Evaluate if level of detail is appropriate to complexity

    5. RELEVANT ADDITIONAL INFORMATION (0.0 to 1.0):
    - If bug mentions: steps to reproduce, environment, logs
        * Does user story preserve or reference this information?
    - Is important business context maintained?
    - Are solution suggestions appropriate?

    Calculate the AVERAGE of the 5 criteria to obtain the final score.

    IMPORTANT:
    - SIMPLE bugs can have high score even without many technical details
    - COMPLEX bugs MUST have additional sections (technical context, tasks, impact)
    - Compare with reference to calibrate completeness expectation

    Return ONLY a valid JSON object in the format:
    {{
    "score": <value between 0.0 and 1.0>,
    "reasoning": "<detailed explanation about what was well covered and what was missing, up to 200 words>"
    }}

    DO NOT add any text before or after the JSON.
    """

    try:
        llm = get_evaluator_llm()
        response = llm.invoke([HumanMessage(content=evaluator_prompt)])
        result = extract_json_from_response(response.content)

        score = float(result.get("score", 0.0))

        return {
            "score": round(score, 4),
            "reasoning": result.get("reasoning", "")
        }

    except Exception as e:
        print(f"Error evaluating Completeness Score: {e}")
        return {
            "score": 0.0,
            "reasoning": f"Evaluation error: {str(e)}"
        }


# Usage example and tests
if __name__ == "__main__":
    # Show configured provider
    provider = os.getenv("LLM_PROVIDER", "openai")
    eval_model = os.getenv("EVAL_MODEL", "gpt-5.2")

    print("=" * 70)
    print("TESTING CUSTOM METRICS")
    print("=" * 70)
    print(f"\nProvider: {provider}")
    print(f"Evaluation Model: {eval_model}\n")

    print("=" * 70)
    print("PART 1: GENERAL METRICS")
    print("=" * 70)

    # Test general metrics
    test_question = "What are the store's opening hours?"
    test_answer = "The store is open Monday to Friday from 9am to 6pm."
    test_reference = "Opening hours: Monday to Friday 9:00-18:00, Saturday 9:00-14:00"

    print("\n1. F1-Score:")
    f1_result = evaluate_f1_score(test_question, test_answer, test_reference)
    print(f"   Score: {f1_result['score']:.2f}")
    print(f"   Precision: {f1_result['precision']:.2f}")
    print(f"   Recall: {f1_result['recall']:.2f}")
    print(f"   Reasoning: {f1_result['reasoning']}\n")

    print("2. Clarity:")
    clarity_result = evaluate_clarity(test_question, test_answer, test_reference)
    print(f"   Score: {clarity_result['score']:.2f}")
    print(f"   Reasoning: {clarity_result['reasoning']}\n")

    print("3. Precision:")
    precision_result = evaluate_precision(test_question, test_answer, test_reference)
    print(f"   Score: {precision_result['score']:.2f}")
    print(f"   Reasoning: {precision_result['reasoning']}\n")

    print("=" * 70)
    print("PART 2: SPECIFIC METRICS FOR BUG TO USER STORY")
    print("=" * 70)

    # Test Bug to User Story specific metrics
    test_bug = "Add to cart button doesn't work on product ID 1234."
    test_user_story = """As a customer browsing the store, I want to add products to my shopping cart, so that I can continue shopping and complete my purchase later.

    Acceptance Criteria:
    - Given that I am viewing a product
    - When I click the "Add to Cart" button
    - Then the product should be added to the cart
    - And I should see a visual confirmation
    - And the cart counter should be updated"""

    test_reference_story = test_user_story  # Using the same for testing

    print("\n4. Tone Score (Professional and empathetic tone):")
    tone_result = evaluate_tone_score(test_bug, test_user_story, test_reference_story)
    print(f"   Score: {tone_result['score']:.2f}")
    print(f"   Reasoning: {tone_result['reasoning']}\n")

    print("5. Acceptance Criteria Score (Criteria quality):")
    criteria_result = evaluate_acceptance_criteria_score(test_bug, test_user_story, test_reference_story)
    print(f"   Score: {criteria_result['score']:.2f}")
    print(f"   Reasoning: {criteria_result['reasoning']}\n")

    print("6. User Story Format Score (Correct format):")
    format_result = evaluate_user_story_format_score(test_bug, test_user_story, test_reference_story)
    print(f"   Score: {format_result['score']:.2f}")
    print(f"   Reasoning: {format_result['reasoning']}\n")

    print("7. Completeness Score (Completeness and context):")
    completeness_result = evaluate_completeness_score(test_bug, test_user_story, test_reference_story)
    print(f"   Score: {completeness_result['score']:.2f}")
    print(f"   Reasoning: {completeness_result['reasoning']}\n")

    print("=" * 70)
    print("ALL TESTS COMPLETED!")
    print("=" * 70)
